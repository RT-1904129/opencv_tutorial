{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facial recognisation by opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "#now we use cascate classifier function used to classify the eye ear and body part\n",
    "face_classifier=cv2.CascadeClassifier(r\"C:\\Users\\Asus\\Downloads\\additional_stdy\\opencv_tutorial\\haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collecting face samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colleting Samples Complete !!!\n"
     ]
    }
   ],
   "source": [
    "def face_extractor(img):\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)#conversion into gray Scale images\n",
    "    faces=face_classifier.detectMultiScale(gray,1.3,5)\n",
    "    #face_classifier.detectMultiScale(images,scaling,minumun neighbourrange(1,6) better to put 5 or 6 for better accuracy)\n",
    "    if faces is () :\n",
    "        return None\n",
    "    for(x,y,w,h) in faces:\n",
    "        cropped_face=img[y:y+h,x:x+w]#it provide a matrix which is used for get only facial matrix else other are remove\n",
    "    return cropped_face\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "count=0\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        count+=1\n",
    "        face=cv2.resize(face_extractor(frame),(200,200))\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)# again convert it into RGB face image\n",
    "        \n",
    "        final_name_path=\"C:/Users/Asus/Downloads/additional_stdy/opencv_tutorial/faces/faces\"+str(count)+\".jpg\"\n",
    "        cv2.imwrite(final_name_path,face)\n",
    "        cv2.putText(face,str(count),(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
    "        # IT is used to write on image cv2.putText(IMAGE,str(count),(starting coordinate,end coordinate),cv2.FONT_HERSHEY_COMPLEX(used for style of sytex ),scaling of font,(B,G,R),thickness of font)\n",
    "        # it count on faces so we can see it on window\n",
    "        cv2.imshow(\"Face Cropper\",face)\n",
    "    else:\n",
    "        print(\"Face not found\")\n",
    "        pass\n",
    "    \n",
    "    if (cv2.waitKey(1)==13) or (count==100):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Colleting Samples Complete !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so our Data set are ready now we focus on proper training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we to train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir#we use it to fetch data from directry\n",
    "from os.path import isfile,join\n",
    "data_path=\"C:/Users/Asus/Downloads/additional_stdy/opencv_tutorial/faces/\"\n",
    "onlyfile=[f for f in listdir(data_path)  if isfile(join(data_path,f))]\n",
    "# if isfile(join(data_path,f)) it check if we join f to data_path than is it file or directory if it is file than take it if it if directory than deteched these file\n",
    "#listdir(data_path)we use it to fetch data from directry\n",
    "#join(data_path,f) it join like that  path data_path/f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faces1.jpg',\n",
       " 'faces10.jpg',\n",
       " 'faces100.jpg',\n",
       " 'faces11.jpg',\n",
       " 'faces12.jpg',\n",
       " 'faces13.jpg',\n",
       " 'faces14.jpg',\n",
       " 'faces15.jpg',\n",
       " 'faces16.jpg',\n",
       " 'faces17.jpg',\n",
       " 'faces18.jpg',\n",
       " 'faces19.jpg',\n",
       " 'faces2.jpg',\n",
       " 'faces20.jpg',\n",
       " 'faces21.jpg',\n",
       " 'faces22.jpg',\n",
       " 'faces23.jpg',\n",
       " 'faces24.jpg',\n",
       " 'faces25.jpg',\n",
       " 'faces26.jpg',\n",
       " 'faces27.jpg',\n",
       " 'faces28.jpg',\n",
       " 'faces29.jpg',\n",
       " 'faces3.jpg',\n",
       " 'faces30.jpg',\n",
       " 'faces31.jpg',\n",
       " 'faces32.jpg',\n",
       " 'faces33.jpg',\n",
       " 'faces34.jpg',\n",
       " 'faces35.jpg',\n",
       " 'faces36.jpg',\n",
       " 'faces37.jpg',\n",
       " 'faces38.jpg',\n",
       " 'faces39.jpg',\n",
       " 'faces4.jpg',\n",
       " 'faces40.jpg',\n",
       " 'faces41.jpg',\n",
       " 'faces42.jpg',\n",
       " 'faces43.jpg',\n",
       " 'faces44.jpg',\n",
       " 'faces45.jpg',\n",
       " 'faces46.jpg',\n",
       " 'faces47.jpg',\n",
       " 'faces48.jpg',\n",
       " 'faces49.jpg',\n",
       " 'faces5.jpg',\n",
       " 'faces50.jpg',\n",
       " 'faces51.jpg',\n",
       " 'faces52.jpg',\n",
       " 'faces53.jpg',\n",
       " 'faces54.jpg',\n",
       " 'faces55.jpg',\n",
       " 'faces56.jpg',\n",
       " 'faces57.jpg',\n",
       " 'faces58.jpg',\n",
       " 'faces59.jpg',\n",
       " 'faces6.jpg',\n",
       " 'faces60.jpg',\n",
       " 'faces61.jpg',\n",
       " 'faces62.jpg',\n",
       " 'faces63.jpg',\n",
       " 'faces64.jpg',\n",
       " 'faces65.jpg',\n",
       " 'faces66.jpg',\n",
       " 'faces67.jpg',\n",
       " 'faces68.jpg',\n",
       " 'faces69.jpg',\n",
       " 'faces7.jpg',\n",
       " 'faces70.jpg',\n",
       " 'faces71.jpg',\n",
       " 'faces72.jpg',\n",
       " 'faces73.jpg',\n",
       " 'faces74.jpg',\n",
       " 'faces75.jpg',\n",
       " 'faces76.jpg',\n",
       " 'faces77.jpg',\n",
       " 'faces78.jpg',\n",
       " 'faces79.jpg',\n",
       " 'faces8.jpg',\n",
       " 'faces80.jpg',\n",
       " 'faces81.jpg',\n",
       " 'faces82.jpg',\n",
       " 'faces83.jpg',\n",
       " 'faces84.jpg',\n",
       " 'faces85.jpg',\n",
       " 'faces86.jpg',\n",
       " 'faces87.jpg',\n",
       " 'faces88.jpg',\n",
       " 'faces89.jpg',\n",
       " 'faces9.jpg',\n",
       " 'faces90.jpg',\n",
       " 'faces91.jpg',\n",
       " 'faces92.jpg',\n",
       " 'faces93.jpg',\n",
       " 'faces94.jpg',\n",
       " 'faces95.jpg',\n",
       " 'faces96.jpg',\n",
       " 'faces97.jpg',\n",
       " 'faces98.jpg',\n",
       " 'faces99.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is Trained\n"
     ]
    }
   ],
   "source": [
    "Training_Data,Labels=[],[]\n",
    "for i,files in enumerate(onlyfile):\n",
    "    image_path=data_path+onlyfile[i]\n",
    "    image=cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "    #WE CAN READ ONLY GRAY SCALE IMAGES as cv2.imread(image_path,cv2.IMREAD_GRAYSCALE) if grayscale images present by deafult imread reads RGB data\n",
    "    Training_Data.append(np.asarray(image,dtype=np.uint8))#here we convert matrix type into array type\n",
    "    Labels.append(i)\n",
    "\n",
    "\n",
    "model=cv2.face.LBPHFaceRecognizer_create()#this model work on gray Scale only\n",
    "model.train(np.asarray(Training_Data),np.asarray(Labels))\n",
    "print(\"Model is Trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output result means predicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier=cv2.CascadeClassifier(r\"C:\\Users\\Asus\\Downloads\\additional_stdy\\opencv_tutorial\\haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def face_detector(img,size=0.5):\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces=face_classifier.detectMultiScale(gray,1.3,5)\n",
    "    if faces is ():#if face is not have id there\n",
    "        return img,[]\n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)#green=255 high intensity,Red=255 high intesity\n",
    "        # it draw rectangle on image cv2.rectangle(img,(start x,start y),(end coordinate of x and y (x+w,y+h)),(color B,G,R),thickness of rectangle)\n",
    "        roi=img[y:y+h,x:x+w]\n",
    "        #roi=region of interest\n",
    "        roi=cv2.resize(roi,(300,300))\n",
    "        \n",
    "    return img,roi\n",
    "\n",
    "cap=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    image,face=face_detector(frame)\n",
    "    try:\n",
    "        face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        result=model.predict(face)# it return a touple(id(image label number equivalent to trained model),confidence(if it close to 0 it is more confient like Vector)))\n",
    "        \n",
    "        if result[1]<500:\n",
    "            confidence=int((1-(result[1])/500)*100)\n",
    "            display_string=str(confidence)+\"% confidence it is user\"\n",
    "        cv2.putText(image,display_string,(200,40),cv2.FONT_HERSHEY_COMPLEX,1,(250,120,120),2)\n",
    "        \n",
    "        if confidence>65:\n",
    "            cv2.putText(image,\"Unlocked\",(250,460),cv2.FONT_HERSHEY_COMPLEX,1,(200,100,10),2)\n",
    "            cv2.imshow(\"Face cropped\",image)\n",
    "        else:\n",
    "            cv2.putText(image,\"locked\",(250,460),cv2.FONT_HERSHEY_COMPLEX,1,(200,100,10),2)\n",
    "            cv2.imshow(\"Face cropped\",image)\n",
    "            \n",
    "    except:\n",
    "        cv2.putText(image,\"Face not found\",(200,40),cv2.FONT_HERSHEY_COMPLEX,1,(250,120,120),2)\n",
    "        cv2.putText(image,\"locked\",(250,460),cv2.FONT_HERSHEY_COMPLEX,1,(200,100,10),2)\n",
    "        cv2.imshow(\"Face cropped\",image)\n",
    "        pass\n",
    "    if cv2.waitKey(1)==13:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
